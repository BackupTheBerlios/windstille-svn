ToDo:
=====

- store 16x16 thumbnails in a container format, simple uncompressed
  RGB, store filenames in a simple linear list, stuff them into a
  map/hash on load

- implement Surface class that handles images larger then the maximum
  texture size

- added support for ~/.thumbnails/ to get the smaller mipmaps faster

- create a two OpenGL context to allow better multithreading use (any
  way to do it portable?)

- move over input code from Pingus to support Spacenavigator and such


Misc Stuff:
===========

1920x1080: (64x64) 30x17=510, (32x32) 60x34=2040, (16x16) 120x68=8160, (4x4) 480x270=129600
~100MB for fullscreen 16x16 Thumbs
4096 images on 1024x1024 texture at 16x16
~32 textures for 129600 thumbs

Problems:
=========
- initial load is very slow

4096 pictures packed in a single JPEG:

  400   /tmp/out-16.jpg
 1161   /tmp/out-32.jpg
 3704   /tmp/out-64.jpg
11776   /tmp/out-128.jpg
37485   /tmp/out-256.jpg

/tmp/out-16.jpg  JPEG   1024x1024 DirectClass  397kb 
/tmp/out-32.jpg  JPEG   2048x2048 DirectClass  1.1mb
/tmp/out-64.jpg  JPEG   4096x4096 DirectClass  3.6mb <- use this as pack format, to slow it seems
/tmp/out-128.jpg JPEG   8192x8192 DirectClass 11.5mb
/tmp/out-256.jpg JPEG 16384x16384 DirectClass 36.6mb

$ time ./thumbget /tmp/out-64.jpg 128 128 64 64 /tmp/test.jpg
/tmp/out-64.jpg 128, 128 - 64x64

real    0m1.022s
user    0m0.876s
sys     0m0.128s

Getting pictures for testing from Flickr:
=========================================

cvs -z3 -d :pserver:anonymous@anoncvs.enlightenment.org:/var/cvs/e export e17/libs/epeg
# Flickr Download:

# Download index files
FUSER=henry-gail for i in $(seq 1 67); do wget -c http://flickr.com/photos/${FUSER}/page${i}/ -O index-${FUSER}-${i}.html; done

# Get photo id's
 grep -i "farm[0-9]" * | sed "s/.*http:/http:/" | cut -c 37-46 | sort -n | uniq > /tmp/ids

 or 

 grep -i "farm[0-9]" * | sed "s/.*http:/http:/" | cut -c 37- | sed "s/_.*//" | grep -v dyic | sort -n | uniq

# download photos
 (for i in $(cat ../ids); do wget -U mozilla -e robots=off -c -p -H   --exclude-domain yahoo.com,yimg.com -p "http://flickr.com/photo_zoom.gne?id=${i}&size=o" --load-cookies /tmp/cookies.txt; done )

- store by md5 and use hardlinks, thus avoid duplicates when files are moved

# EOF #
